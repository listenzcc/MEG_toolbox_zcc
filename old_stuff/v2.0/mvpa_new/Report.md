<h1>MVPA comparison</h1>

<h2>SVM_xdawn_1</h2><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1-f1-score</th>
      <th>1-precision</th>
      <th>1-recall</th>
      <th>2-f1-score</th>
      <th>2-precision</th>
      <th>2-recall</th>
      <th>4-f1-score</th>
      <th>4-precision</th>
      <th>4-recall</th>
      <th>accuracy</th>
      <th>macro avg-f1-score</th>
      <th>macro avg-precision</th>
      <th>macro avg-recall</th>
      <th>weighted avg-f1-score</th>
      <th>weighted avg-precision</th>
      <th>weighted avg-recall</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
      <td>76.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.851995</td>
      <td>0.876727</td>
      <td>0.833629</td>
      <td>0.582882</td>
      <td>0.470061</td>
      <td>0.783211</td>
      <td>0.758261</td>
      <td>0.892545</td>
      <td>0.667748</td>
      <td>0.703691</td>
      <td>0.731046</td>
      <td>0.746444</td>
      <td>0.761529</td>
      <td>0.717690</td>
      <td>0.785006</td>
      <td>0.703691</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.093995</td>
      <td>0.084657</td>
      <td>0.115058</td>
      <td>0.075200</td>
      <td>0.079481</td>
      <td>0.094272</td>
      <td>0.085609</td>
      <td>0.039578</td>
      <td>0.108935</td>
      <td>0.078730</td>
      <td>0.075508</td>
      <td>0.055407</td>
      <td>0.076886</td>
      <td>0.077898</td>
      <td>0.042899</td>
      <td>0.078730</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.592593</td>
      <td>0.615385</td>
      <td>0.517857</td>
      <td>0.438393</td>
      <td>0.285208</td>
      <td>0.514388</td>
      <td>0.338284</td>
      <td>0.799479</td>
      <td>0.207490</td>
      <td>0.401734</td>
      <td>0.481115</td>
      <td>0.593460</td>
      <td>0.557469</td>
      <td>0.376164</td>
      <td>0.678935</td>
      <td>0.401734</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.802679</td>
      <td>0.830328</td>
      <td>0.784740</td>
      <td>0.530262</td>
      <td>0.418525</td>
      <td>0.724344</td>
      <td>0.734728</td>
      <td>0.862087</td>
      <td>0.634364</td>
      <td>0.671607</td>
      <td>0.693907</td>
      <td>0.711160</td>
      <td>0.722204</td>
      <td>0.693026</td>
      <td>0.753199</td>
      <td>0.671607</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.876731</td>
      <td>0.898980</td>
      <td>0.857143</td>
      <td>0.573902</td>
      <td>0.463028</td>
      <td>0.796702</td>
      <td>0.769419</td>
      <td>0.897744</td>
      <td>0.679011</td>
      <td>0.707513</td>
      <td>0.732682</td>
      <td>0.745810</td>
      <td>0.767835</td>
      <td>0.722871</td>
      <td>0.783087</td>
      <td>0.707513</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.919278</td>
      <td>0.941731</td>
      <td>0.927597</td>
      <td>0.647380</td>
      <td>0.527980</td>
      <td>0.853225</td>
      <td>0.811755</td>
      <td>0.929029</td>
      <td>0.741464</td>
      <td>0.758036</td>
      <td>0.783721</td>
      <td>0.786975</td>
      <td>0.823840</td>
      <td>0.769120</td>
      <td>0.821325</td>
      <td>0.758036</td>
    </tr>
    <tr>
      <th>max</th>
      <td>0.990991</td>
      <td>1.000000</td>
      <td>0.982143</td>
      <td>0.725424</td>
      <td>0.623301</td>
      <td>0.947059</td>
      <td>0.864294</td>
      <td>0.960818</td>
      <td>0.815657</td>
      <td>0.825340</td>
      <td>0.860236</td>
      <td>0.854280</td>
      <td>0.883305</td>
      <td>0.832592</td>
      <td>0.858207</td>
      <td>0.825340</td>
    </tr>
  </tbody>
</table>

